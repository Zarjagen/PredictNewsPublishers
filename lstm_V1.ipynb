{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, copy, torch, string, re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torch import nn, optim, cuda\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the minimum number of articles for publication is 5214, and maximum is 11488\n",
    "num_train = 80 # articles for each publication\n",
    "num_valid = 20 # articles for each publication\n",
    "num_test = 20 # articles for each publication\n",
    "data_shuffle = True # If false, always returns the first n articles for each publication\n",
    "\n",
    "rnn_type = 'LSTM'\n",
    "embed_size = 500\n",
    "vocab_size = 10000\n",
    "hidden_size = 1000\n",
    "hidden_layer = 1\n",
    "dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllTheNews(Dataset):\n",
    "    def __init__(self, df_map, labels_map, words_map, start, end):\n",
    "\n",
    "        self.df_map = df_map\n",
    "        self.labels_map = labels_map\n",
    "        self.words_map = words_map\n",
    "        \n",
    "        self.df = pd.DataFrame()\n",
    "        \n",
    "        for key, info in df_map.items():\n",
    "            self.df = self.df.append(info[start:min(end, len(info))])\n",
    "        \n",
    "        self.max_len = 0\n",
    "        for article in self.df['content']:\n",
    "            self.max_len = max(len(article.split()), self.max_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, article = self.df.iloc[idx]\n",
    "        article = self.tokenize(article)\n",
    "        label = torch.Tensor([labels_map[label]])\n",
    "        \n",
    "        sample = (article, label)\n",
    "        return sample\n",
    "    \n",
    "    def tokenize(self, content):\n",
    "        article = torch.zeros(self.max_len).long()\n",
    "        for i, word in enumerate(content.split()):\n",
    "            if word in self.words_map:\n",
    "                article[i] = self.words_map[word]\n",
    "        return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maps():    \n",
    "    df = pd.read_csv(\"Data/articles1.csv\", usecols=['publication', 'content'])\n",
    "    df = df.append(pd.read_csv(\"Data/articles2.csv\", usecols=['publication', 'content']))\n",
    "    df = df.append(pd.read_csv(\"Data/articles3.csv\", usecols=['publication', 'content']))\n",
    "    def remove_all_nonchr(s):\n",
    "        return re.sub(\"[^a-zA-Z]\", \" \", s).lower()\n",
    "    df['content'] = df['content'].apply(remove_all_nonchr)\n",
    "    \n",
    "    if os.path.exists('words_map.txt'):\n",
    "        words_map = {}\n",
    "        for l in open('words_map.txt'):\n",
    "            w, i = l.strip().split(',')\n",
    "            words_map[w] = int(i)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(stop_words = None)\n",
    "        words_matrix = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "        words = vectorizer.get_feature_names()\n",
    "        count = np.squeeze(np.asarray(words_matrix.sum(0)))\n",
    "\n",
    "        words_count = {w:c for w, c in zip(words, count)}\n",
    "        words_chosen = sorted(words_count, key=words_count.get, reverse=True)[:vocab_size-1]\n",
    "\n",
    "        words_map = {word:i+1 for i, word in enumerate(words_chosen)}\n",
    "    \n",
    "    # shuffle whole data set\n",
    "    if data_shuffle:\n",
    "        df = shuffle(df)\n",
    "    df_unique = df.drop_duplicates(subset=['publication'])\n",
    "    # classify into each category by publication\n",
    "    labels = [name for name in df_unique['publication']]\n",
    "    labels_map = {name:idx for idx, name in enumerate(sorted(labels))}\n",
    "    \n",
    "    df_map = {}\n",
    "    for key in labels_map:\n",
    "        df_map[key] = df[df['publication'].isin([key])]\n",
    "    return df_map, labels_map, words_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map, labels_map, words_map = get_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atn_train = AllTheNews(df_map, labels_map, words_map, 0, num_train)\n",
    "atn_valid = AllTheNews(df_map, labels_map, words_map, num_train, num_train+num_valid)\n",
    "atn_test = AllTheNews(df_map, labels_map, words_map, num_train+num_valid, num_train+num_valid+num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = {x:16 for x in ['train', 'valid', 'test']}\n",
    "num_examples = {'train':len(atn_train), 'valid':len(atn_valid), 'test':len(atn_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(\n",
    "    dataset = atn_train,\n",
    "    batch_size = batch_sizes['train'],\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "valid_data = DataLoader(\n",
    "    dataset = atn_valid,\n",
    "    batch_size = batch_sizes['valid'],\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_data = DataLoader(\n",
    "    dataset = atn_test,\n",
    "    batch_size = batch_sizes['test'],\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train':train_data,\n",
    "    'valid':valid_data,\n",
    "    'test':test_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.set_device(0)\n",
    "use_gpu = cuda.is_available()\n",
    "print(\"GPU Availability = {c}\".format(c=use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, rnn_type, vocab_size, embed_size, hidden_size, hidden_layer, output_size, dropout=0.0):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = getattr(nn, rnn_type)(embed_size, hidden_size, hidden_layer, dropout=dropout)\n",
    "#         self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x_embed = self.encoder(x).view(x.size(1), batch_size, -1)\n",
    "\n",
    "        output, hidden = self.rnn(x_embed, hidden)\n",
    "        output = self.fc1(output[-1])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(torch.zeros(self.hidden_layer, batch_size, self.hidden_size).cuda()),\n",
    "                    Variable(torch.zeros(self.hidden_layer, batch_size, self.hidden_size).cuda()))\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            return Variable(torch.zeros(self.hidden_layer, batch_size, self.hidden_size).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = CustomRNN(rnn_type, len(words_map), embed_size, hidden_size, hidden_layer, len(labels_map), dropout=dropout)\n",
    "rnn = rnn.cuda() if use_gpu else rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100.0\n",
    "    \n",
    "    loss_trace = {x:[] for x in ['train', 'valid']}\n",
    "    acc_trace = {x:[] for x in ['train', 'valid']}\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "                \n",
    "        for phase in ['train', 'valid', 'test']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for data in datasets[phase]:\n",
    "                inputs, labels = data\n",
    "                labels = labels.long().view(-1)\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                \n",
    "                hidden = model.initHidden(batch_sizes[phase])\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                outputs, hidden = model(inputs, hidden)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.data[0] * batch_sizes[phase]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / num_examples[phase]\n",
    "            epoch_acc = running_corrects / num_examples[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            loss_trace[phase].append(epoch_loss)\n",
    "            acc_trace[phase].append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                file_path = 'model_{}_{}_{}_{}_{}'.format(\n",
    "                    int(dropout*10), rnn_type, hidden_size, hidden_layer, optimization[:3])\n",
    "                if os.path.exists(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "                print(\"Model saved as {}\".format(file_path))\n",
    "                \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, loss_trace, acc_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn, loss_trace, valid_trace = train_model(rnn, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
