{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all csv files\n",
    "df = pd.read_csv(\"Orig_data/articles1.csv\", usecols=['publication', 'content'])\n",
    "df = df.append(pd.read_csv(\"Orig_data/articles2.csv\", usecols=['publication', 'content']))\n",
    "df = df.append(pd.read_csv(\"Orig_data/articles3.csv\", usecols=['publication', 'content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(normalizeString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_art_len = 50\n",
    "max_sen_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(content):\n",
    "    sents = sent_tokenize(content)\n",
    "    if len(sents) < max_art_len:\n",
    "        sents += [''] * (max_art_len - len(sents))\n",
    "    else:\n",
    "        sents = sents[:max_art_len]\n",
    "    for i, sent in enumerate(sents):\n",
    "        words = word_tokenize(sent)\n",
    "        sents[i] = []\n",
    "        for word in words:\n",
    "            if word in words_map:\n",
    "                sents[i].append(words_map[word])\n",
    "        if len(sents[i]) < max_sen_len:\n",
    "            sents[i] += [0] * (max_sen_len - len(sents[i]))\n",
    "        else:\n",
    "            sents[i] = sents[i][:max_sen_len]\n",
    "    s = ''\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            s += '{} '.format(word)\n",
    "        s += '$ '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in df['content']:\n",
    "    a = tokenize(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all publications\n",
    "df_unique = df.drop_duplicates(subset=['publication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = None,\n",
    "    preprocessor = None,\n",
    "    stop_words = None,\n",
    "    max_features = 19999,\n",
    "    ngram_range = (1, 1),\n",
    ")\n",
    "words_matrix = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "count = np.squeeze(np.asarray(words_matrix.sum(0)))\n",
    "\n",
    "words_count = {w:c for w, c in zip(words, count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_chosen = sorted(words_count, key=words_count.get, reverse=True)\n",
    "words_map = {word:i+1 for i, word in enumerate(words_chosen)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = df['content'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make labels\n",
    "labels = [name for name in df_unique['publication']]\n",
    "labels_map = {name:idx for idx, name in enumerate(sorted(labels))}\n",
    "# create label-aware map\n",
    "df_map = {}\n",
    "for key in labels_map:\n",
    "    df_map[key] = df[df['publication'].isin([key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in labels_map:\n",
    "    del df_map[key]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump articles for different publications into different csv files\n",
    "if not os.path.exists('Data'):\n",
    "    os.makedirs('Data')\n",
    "for publication in df_map:\n",
    "    df_map[publication].to_csv(os.path.join('Data', '{}.gz'.format(publication)), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Map/labels_map.txt', 'w') as f:\n",
    "    for k, v in sorted(labels_map.items()):\n",
    "        f.write('{},{}\\n'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Map/words_map.txt', 'w') as f:\n",
    "    for k in sorted(words_map, key=words_map.get):\n",
    "        f.write('{},{}\\n'.format(k, words_map[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
